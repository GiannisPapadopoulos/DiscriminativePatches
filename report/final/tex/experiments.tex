\section{Experiments}
\nocite{Kanade2000CK+}\nocite{Lucey2010CK+}

TODO: describe setup of datasets sued for training and validation.Present results

\subsection{Data}
For training a system to recognize facial expression effectively, the dataset should focus on several main expressions including anger, disgust, fear, happiness, sadness and surprise. Moreover, a proper dataset should be composed of faces with kinds of face shapes, colors, facial and scalp hairs from many participants with different genders, ethnic backgrounds and ages. 
\\
\\
The dataset this paper adopted is from the Cohn-Kanade Facial Expression Database. The dataset refers to eight emotions including anger, contempt, disgust, fear, happiness, sadness, surprise and neutral expression. There are 5105 images from 123 subjects, who ranged in age from 18 to 30 years. Sixty-five percent are female, eight-five percent are Euro-American and fifteen percent are African-American and Asian. They were observed in an observation room equipped with a chair on which to sit and a camera was located directly in front of the subject. Therefore all images have the uniform background and lighting. The images were digitized into 640*480 or 490 pixel arrays with 8-bit precision for grayscale values and are available in png and jpg. 
\\
\\
In the Cohn-Kanade Facial Expression Dataset, subjects performed a series of several facial displays from neutral expressions to peak expressions and images were taken frame by frame. In order to train a powerful system, we chose neutral faces and obvious expressions. That means there is no indistinguishable expression in our dataset. Figure 1 shows some example of the dataset.



\begin{figure}[h!]
\centering
\includegraphics[scale=0.55]{img/example.png}
\caption{Example of dataset}
\label{Example of dataset}
\end{figure}

\subsection{Experiment setup}
The experiments were performed doing one versus all classification. The data for each expression is divided into a training and a validation set and an SVM
classifier is calculated based on the training set. The accuracy of the classifiers is then measured on the validation set. It was ensured that images of the same
person are not present in both the training and validation set. The fraction of the data used for the validation set is a configurable parameter, for the 
following experiments 75\% of the data was used for the training set and 25\% for the valdiation set. 

\subsection{Manual Patches}

An image can have multiple discriminating patches. As shown in Figure \ref{fig:manual_patch}, some patches are highly noticeably discriminating such an open mouth and raised eyebrows in "`surprise"' expression or a wrinkled glabella (area between eyes) in "`disgust"' expression. Such patches can be the best approximation of most discriminating patches. Hence, they make a good basis for testing the algorithm. 

\begin{figure}
\centering
\includegraphics[width=200pt]{img/manual_patch.png}
  \caption{Noticeably discriminating patches}
  \label{fig:manual_patch}
\end{figure}

We identified four such patches, that 2 patches for eyes and one each for mouth and glabella. We extracted them manually and and it was ensured that extracted images of same patch are consistent with each other in terms feature content of the patch. For example, in case of mouth patches, it was ensured that the boundaries of the patch touch the edges of lips in all mouth patches.

Tables \ref{table:left_eye}, \ref{table:right_eye}, \ref{table:between_eyes} and \ref{table:mouth} show the results for each region, for classifiers trained 
with the original patch size of 96x96 and also rescaled to 32x32 and 64x64 pixels. Shrinking the images on the one hand results in fewer hog features which is likely to reduce the effect of  verfitting, on the other hand some visual information may be lost. The table entries represent the fraction of the images of the corresponding expression that were correctly classified, in the 0 to 1 range.



%\subsection{Results}

\begin{table}
\caption{Left eye patches}
\label{table:left_eye}

\begin{tabular}{| c | c | c | c |}
\hline
Expression & 32 x 32 &  64 x 64  & 96 x 96  \\

\hline
Angry & 0.5865 & 0.6786 & 0.7068 \\
Contempt & 0.6596 &	0.5745 & 0.5532 \\
Disgust	& 0.7629 &	0.7629 &	0.7716 \\
Fear &	0.6155 & 0.6315 & 0.6394 \\ 
Happy &	0.6074 & 0.6605 & 0.6529 \\ 
Neutral & 0.6851 &	0.6996 & 0.7093 \\
Sadness & 0.5207 & 0.5041 &	0.5021 \\
Surprise & 0.7652 &	0.7683 & 0.7561 \\

\hline
\end{tabular}
\end{table}

\begin{table}
\caption{Right eye patches}
\label{table:right_eye}

\begin{tabular}{| c | c | c | c |}
\hline
Expression & 32 x 32 &  64 x 64  & 96 x 96  \\

\hline
Angry    & 0.5733 & 0.6466 & 0.6654 \\
Contempt & 0.5638 & 0.5106 & 0.6170 \\ 
Disgust	 & 0.7263 & 0.7414 & 0.7155 \\
Fear	 & 0.6474 & 0.6076 & 0.6096 \\
Happy	 & 0.6367 & 0.6334 & 0.6833 \\
Neutral  & 0.7099 & 0.7272 & 0.7562 \\
Sadness  & 0.5000 & 0.5207 & 0.5456 \\
Surprise & 0.7896 & 0.8171 & 0.8079 \\

\hline
\end{tabular}
\end{table}

\begin{table}
\caption{Part between eyes patches}
\label{table:between_eyes}

\begin{tabular}{| c | c | c | c |}
\hline
Expression & 32 x 32 &  64 x 64  & 96 x 96  \\

\hline
Angry & 0.7971 & 0.812 & 0.7857 \\
Contempt & 0.4468 & 0.4574 & 0.4681 \\
Disgust & 0.6983 & 0.7155 & 0.7306 \\
Fear & 0.7071 & 0.7649 & 0.7351 \\
Happy & 0.8503 & 0.8503 & 0.8329 \\
Neutral & 0.7438 & 0.7845 & 0.7921 \\
Sadness & 0.7324 & 0.7697 & 0.7635 \\
Surprise & 0.7317 & 0.7485 & 0.7165 \\

\hline
\end{tabular}
\end{table}


\begin{table}
\caption{Mouth patches}
\label{table:mouth}

\begin{tabular}{| c | c | c | c |}
\hline
Expression & 32 x 32 &  64 x 64  & 96 x 96  \\

\hline
Angry	&	0.7782	&	0.8064	&	0.7970	\\
Contempt	&	0.6277	&	0.6598	& 0.6702 \\
Disgust	&	0.6918	&	0.7953	&	0.8039	\\
Fear	&	0.7510	&	0.8287	&	0.8307	\\
Happy	&	0.8959	&	0.9121	&	0.9154	\\
Neutral	&	0.7974	&	0.8577	&	0.8501	\\
Sadness	&	0.8589	&	0.8651	&	0.8693	\\
Surprise &	0.8975	&	0.9146	&	0.8959	\\

\hline
\end{tabular}
\end{table}

\subsection{SVM on the entire facial images}
We also experimented with training SVM classifiers considering the entire facial area as a single patch. First the bounding box of the face was detected
for each image using the Viola-Jones algorithm, then rescaled to a uniform size. % TODO: Add Reference. 
The rest of the procedure was identical to the preceding experiment. The results can be found in Table \ref{table:entire_images}.

\begin{table}
\caption{Classifying the whole image}
\label{table:entire_images}

\begin{tabular}{| c | c | c | c |}
\hline
Expression & 32 x 32 &  64 x 64  & 96 x 96  \\

\hline
Angry	 & 0.6165 & 0.6316 & 0.6297	\\
Contempt & 0.4894 & 0.5000 & 0.4043	\\
Disgust	 & 0.7109 & 0.8793 & 0.9367	\\
Fear	 & 0.3540 & 0.6574 & 0.6420	\\
Happy	 & 0.8080 & 0.9317 & 0.9360	\\
Neutral	 & 0.7203 & 0.8260 & 0.8225	\\
Sadness	 & 0.5996 & 0.5830 & 0.6432	\\
Surprise & 0.8765 & 0.9405 & 0.9665	\\

\hline
\end{tabular}
\end{table}


\subsection{Results of Prediction}
After training SVM classifier, we needed to know whether the classifier is able to predict the training data. We picked out all patches (the part between eyes, left eye, right eye and mouth) from one subject with complete time series of almost emotions. These patches were predicted respectively after being rescaled to 96*96 pixels in the process of training. 
\\

Table \ref{table:predict_mouth} shows the result of prediction of mouth patches. Most parts of mouth patches were given correct classifications. However, for angry, fear, happy and surprise emotion, there are one or two ambiguous facial expressions so that the patches of them were classified to an incorrect emotion. By contrast, the prediction of right-eye patches was not as good as that of mouth patches. As Table\ref{table:predict_righteye} shows, right-eye patches of angry and fear emotions were not predicted correctly. For other emotions, the accuracy of the prediction of right-eye patches was lower than that of mouth patches.

\begin{table}
\caption{Predicting the Mouth Patches}
\label{table:predict_mouth}

\begin{tabular}{| c | c | c | c | c | c | c |}
\hline
Time & Angry &  Disgust  & Fear & Happy & Sadness & Surprise  \\
\hline
1 & neutral & neutral & neutral & neutral & neutral & neutral \\
2 & neutral & neutral & neutral & neutral & neutral & neutral \\
3 & neutral & neutral & neutral & neutral & neutral & neutral \\
4 & neutral & neutral & neutral & disgust & neutral & neutral \\
5 & neutral & neutral & neutral & happy & neutral & neutral \\
6 & neutral & neutral & disgust & happy & neutral & disgust \\
7 & neutral & disgust & fear & happy & neutral & surprise \\
8 & neutral & disgust & fear & happy & neutral & surprise \\
9 & neutral & disgust & fear & happy & sadness & surprise \\
10 & contempt & disgust & fear & happy & sadness & surprise \\
11 & neutral & disgust & fear & happy & sadness & surprise \\
12 & contempt & disgust & fear & happy & sadness & surprise \\
13 & angry &  & fear &  & sadness & surprise \\
14 & angry &  & fear &  & sadness & surprise \\
15 & angry &  &  &  & sadness & \\

\hline
\end{tabular}
\end{table}


\begin{table}
\caption{Predicting the Right-Eye Patches}
\label{table:predict_righteye}

\begin{tabular}{| c | c | c | c | c | c | c |}
\hline
Time & Angry &  Disgust  & Fear & Happy & Sadness & Surprise  \\
\hline
1 & disgust & neutral & neutral & neutral & neutral & neutral \\
2 & neutral & disgust & neutral & neutral & neutral & neutral \\
3 & neutral & neutral & neutral & neutral & neutral & neutral \\
4 & disgust & neutral & neutral & contempt & neutral & neutral \\
5 & neutral & contempt & neutral & contempt & neutral & neutral	\\
6 & happy & contempt & neutral & contempt & contempt & neutral \\
7 & contempt & disgust & neutral & happy & contempt & neutral \\
8 & neutral & disgust & contempt & happy & neutral & neutral \\
9 & contempt & disgust & happy & happy & sadness & surprise \\
10 & contempt & disgust & contempt & happy & sadness & surprise	\\
11 & contempt & disgust & contempt & happy & sadness & surprise \\
12 & neutral & disgust & neutral & happy & sadness & surprise \\
13 & contempt &  & contempt &  & sadness & surprise \\
14 & contempt &  & neutral &  & sadness & surprise \\
15 & neutral &  &  &  & sadness & \\

\hline
\end{tabular}
\end{table}


\section{Discussion}
For the manually extracted patches the best overall results were obtained for the mouth region, with over 80\% of the images classified correctly.
This confirms our intuition as this region is the most distinguishable
from a human perspective. Results for the eye regions were better than expected, as visually the extracted patches look very similar. For the disgust emotion all 
four regions lead to comparable results, while for the anger emotion the region between the eyes is almost as discriminative as the mouth. 
Rescaling the images does not appear to have a very significant effect. 

Performing SVM classification on the entire facial region led to good results if the region was rescaled to 96x96 pixels, better than all manually extracted 
patches except the mouth. In this case there is a clear loss
of precision if the image is rescaled to 32x32 pixels. % TODO Run the experiment without face detection to measure the effect, discuss the original paper



