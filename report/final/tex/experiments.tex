\section{Experiments}
\nocite{Kanade2000CK+}\nocite{Lucey2010CK+}

TODO: describe setup of datasets sued for training and validation.Present results

\subsection{Data}
For training a system to recognize facial expression effectively, the dataset should focus on several main expressions including anger, disgust, fear, happiness, sadness and surprise. Moreover, a proper dataset should be composed of faces with kinds of face shapes, colors, facial and scalp hairs from many participants with different genders, ethnic backgrounds and ages. 
\\
\\
The dataset this paper adopted is from the Cohn-Kanade Facial Expression Database\cite{Kanade2000CK+}. The dataset refers to eight emotions including anger, contempt, disgust, fear, happiness, sadness, surprise and neutral expression. There are 5105 images from 123 subjects, who ranged in age from 18 to 30 years. Sixty-five percent are female, eight-five percent are Euro-American and fifteen percent are African-American and Asian. They were observed in an observation room equipped with a chair on which to sit and a camera was located directly in front of the subject. Therefore all images have the uniform background and lighting. The images were digitized into 640*480 or 490 pixel arrays with 8-bit precision for grayscale values and are available in png and jpg. 
\\
\\
In the Cohn-Kanade Facial Expression Dataset, subjects performed a series of several facial displays from neutral expressions to peak expressions and images were taken frame by frame. In order to train a powerful system, we chose neutral faces and obvious expressions. That means there is no indistinguishable expression in our dataset. Figure 1 shows some example of the dataset.



\begin{figure}[h!]
\centering
\includegraphics[scale=0.55]{img/example.png}
\caption{Example of dataset}
\label{Example of dataset}
\end{figure}

\subsection{Experiment setup}
The experiments were performed doing one versus all classification. The data for each expression is divided into a training and a validation set and an SVM
classifier is calculated based on the training set. The accuracy of the classifiers is then measured on the validation set. It was ensured that images of the same
person are not present in both the training and validation set. The fraction of the data used for the validation set is a configurable parameter, for the 
following experiments 75\% of the data was used for the training set and 25\% for the valdiation set. 

\subsection{Manual Patches}

An image can have multiple discriminating patches. As shown in Figure \ref{fig:manual_patch}, some patches are highly noticeably discriminating such an open mouth and raised eyebrows in "`surprise"' expression or a wrinkled glabella (area between eyes) in "`disgust"' expression. Such patches can be the best approximation of most discriminating patches. Hence, they make a good basis for testing the algorithm. 

\begin{figure}
\centering
\includegraphics[width=200pt]{img/manual_patch.png}
  \caption{Noticeably discriminating patches}
  \label{fig:manual_patch}
\end{figure}

We identified four such patches, that 2 patches for eyes and one each for mouth and glabella. We extracted them manually and and it was ensured that extracted images of same patch are consistent with each other in terms feature content of the patch. For example, in case of mouth patches, it was ensured that the boundaries of the patch touch the edges of lips in all mouth patches.

Tables \ref{table:left_eye}, \ref{table:right_eye}, \ref{table:between_eyes} and \ref{table:mouth} show the results for each region, for classifiers trained 
with the original patch size of 96x96 and also rescaled to 32x32 and 64x64 pixels. Shrinking the images on the one hand results in fewer hog features which is likely to reduce the effect of  verfitting, on the other hand some visual information may be lost. The table entries represent the fraction of the images of the corresponding expression that were correctly classified, in the 0 to 1 range.



%\subsection{Results}

\begin{table}
\caption{Left eye patches}
\label{table:left_eye}

\begin{tabular}{| c | c | c | c |}
\hline
Expression & 32 x 32 &  64 x 64  & 96 x 96  \\

\hline
Angry & 0.5865 & 0.6786 & 0.7068 \\
Contempt & 0.6596 &	0.5745 & 0.5532 \\
Disgust	& 0.7629 &	0.7629 &	0.7716 \\
Fear &	0.6155 & 0.6315 & 0.6394 \\ 
Happy &	0.6074 & 0.6605 & 0.6529 \\ 
Neutral & 0.6851 &	0.6996 & 0.7093 \\
Sadness & 0.5207 & 0.5041 &	0.5021 \\
Surprise & 0.7652 &	0.7683 & 0.7561 \\

\hline
\end{tabular}
\end{table}

\begin{table}
\caption{Right eye patches}
\label{table:right_eye}

\begin{tabular}{| c | c | c | c |}
\hline
Expression & 32 x 32 &  64 x 64  & 96 x 96  \\

\hline
Angry    & 0.5733 & 0.6466 & 0.6654 \\
Contempt & 0.5638 & 0.5106 & 0.6170 \\ 
Disgust	 & 0.7263 & 0.7414 & 0.7155 \\
Fear	 & 0.6474 & 0.6076 & 0.6096 \\
Happy	 & 0.6367 & 0.6334 & 0.6833 \\
Neutral  & 0.7099 & 0.7272 & 0.7562 \\
Sadness  & 0.5000 & 0.5207 & 0.5456 \\
Surprise & 0.7896 & 0.8171 & 0.8079 \\

\hline
\end{tabular}
\end{table}

\begin{table}
\caption{Part between eyes patches}
\label{table:between_eyes}

\begin{tabular}{| c | c | c | c |}
\hline
Expression & 32 x 32 &  64 x 64  & 96 x 96  \\

\hline
Angry & 0.7971 & 0.812 & 0.7857 \\
Contempt & 0.4468 & 0.4574 & 0.4681 \\
Disgust & 0.6983 & 0.7155 & 0.7306 \\
Fear & 0.7071 & 0.7649 & 0.7351 \\
Happy & 0.8503 & 0.8503 & 0.8329 \\
Neutral & 0.7438 & 0.7845 & 0.7921 \\
Sadness & 0.7324 & 0.7697 & 0.7635 \\
Surprise & 0.7317 & 0.7485 & 0.7165 \\

\hline
\end{tabular}
\end{table}


\begin{table}
\caption{Mouth patches}
\label{table:mouth}

\begin{tabular}{| c | c | c | c |}
\hline
Expression & 32 x 32 &  64 x 64  & 96 x 96  \\

\hline
Angry	&	0.7782	&	0.8064	&	0.7970	\\
Contempt	&	0.6277	&	0.6598	& 0.6702 \\
Disgust	&	0.6918	&	0.7953	&	0.8039	\\
Fear	&	0.7510	&	0.8287	&	0.8307	\\
Happy	&	0.8959	&	0.9121	&	0.9154	\\
Neutral	&	0.7974	&	0.8577	&	0.8501	\\
Sadness	&	0.8589	&	0.8651	&	0.8693	\\
Surprise &	0.8975	&	0.9146	&	0.8959	\\

\hline
\end{tabular}
\end{table}

\subsection{SVM on the entire facial images}
We also experimented with training SVM classifiers considering the entire facial area as a single patch. First the bounding box of the face was detected
for each image using the Viola-Jones algorithm, then rescaled to a uniform size. % TODO: Add Reference. 
The rest of the procedure was identical to the preceding experiment. The results can be found in Table \ref{table:entire_images}.

\begin{table}
\caption{Classifying the whole image}
\label{table:entire_images}

\begin{tabular}{| c | c | c | c |}
\hline
Expression & 32 x 32 &  64 x 64  & 96 x 96  \\

\hline
Angry	 & 0.6165 & 0.6316 & 0.6297	\\
Contempt & 0.4894 & 0.5000 & 0.4043	\\
Disgust	 & 0.7109 & 0.8793 & 0.9367	\\
Fear	 & 0.3540 & 0.6574 & 0.6420	\\
Happy	 & 0.8080 & 0.9317 & 0.9360	\\
Neutral	 & 0.7203 & 0.8260 & 0.8225	\\
Sadness	 & 0.5996 & 0.5830 & 0.6432	\\
Surprise & 0.8765 & 0.9405 & 0.9665	\\

\hline
\end{tabular}
\end{table}
