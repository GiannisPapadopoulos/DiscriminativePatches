
\section{Technical Approach}\label{sec:StateArt}

This section gives an explanation of definitions used and an overview of the algorithm proposed by Singh et Al. \cite{Singh2012DiscPat} and the image processing techniques used therein.

\subsection{Definition of Image Patches}

To explain the notion of mid-level discriminative patches, it is best to first observe the different levels of visual information that can be retrieved from an image. From a bottom-up point-of-view, the lowest level corresponds to a single pixel, but single pixels do obviously not provide much useful information to describe features of the real world. On the opposite side, the highest level corresponds to the image as a whole. This poses several problems on the analysis of the retrieved information such as a high number of spatial configurations needed to describe objects. Also the image as a whole contains too much unnecessary information to describe certain features on their own. As Sing et Al. pointed out \cite{Singh2012DiscPat}, the optimal level of information is found by looking at medium-sized parts of the image that describe one certain feature of an image. Such parts are called mid-level image patches. Singh et Al. \cite{Singh2012DiscPat} define such patches as being discriminative if they clearly represent the described feature and if they are different enough from other patches describing the same or any other feature. Additionally, they require such patches to be detectable with "high recall and precision" in a large number of images.

\subsection{Classification of Image Patches}

The algorithm to detect discriminative patches in an unsupervised manner mainly consists of three parts: The first one is to extract features from an image, then cluster these features are  and for each cluster train a linear support vector machine (SVM) finally used to classify unknown images. In terms of classification of facial expression for example, a feature that distinguishes different emotional expressions can be the mouth. Happiness is expressed by smiling and showing the teeth, while surprise is usually expressed with a wide open mouth. In this example the mouth parts of an image will be extracted and analyzed. The obtained data from this feature extraction analysis is then used to train SVMs for different emotions, in case of happiness taking the smiling mouths as positive examples and examples from other emotions as negative samples.
\\
\\
Technically, the feature extraction is performed by using the Histogram of Oriented Gradients (HOG) presented by Dalal and Triggs \cite{Dalal:2005:HOG:1068507.1069007}. The features are computed as intensity gradients for small parts of an image. The computed edge directions are then counted and concatenated to form a comprehensive descriptor. One of the key advantages of this technique is that it is invariant to geometric or photometric transformations. Dalal and Triggs showed \cite{Dalal:2005:HOG:1068507.1069007}, that the HOG features produce good results when they are used for classification of images. The initial classification step then is performed by clustering the features using k-means clustering. This promising starting point for image classification has been presented by Coates et Al. \cite{DBLP:series/lncs/CoatesN12}. K-means clustering, introduced in \cite{macqueen1967}, partitions a dataset into a given number of clusters in such a way that the sum of the quadratic deviations from each cluster centroid is minimized. As metric usually the Euclidian distance is used. The actual classification finally is performed by a linear SVM. A linear SVM separates points with a hyperplane that has a maximized distance to the nearest data points on each side of the plane. This can be used as binary classifier to classify input data according to recognized patterns as shown by Chang et Al. \cite{Chang:2011:LLS:1961189.1961199}. These patterns are obtained by training the SVM on positive and negative samples of data describing the problem that needs to be solved by classification. The training of the classifier is done one-vs-all. In terms of this project, this means, that the patches extracted for one certain emotion are used as positive samples and an equal number of samples of patches extracted in an evenly distributed manner from all other emotions is used as negative counterpart for the training of the classifiers. The equal number of positive and negative samples ensures a well-balanced classification. Otherwise a larger number of one of the two parts, positive or negative, could influence the outcome of the prediction for validation data in the direction of this larger part.

\subsection{Unsupervised Detection of Discriminative Patches}

Applying these techniques, the feature extraction using HOG descriptors produces a large number of possibly overlapping patches at multiple scales. From these patches extracted from all over the image, the most discriminative ones have to be obtained automatically. This is done by combining the clustering using k-means and the classification using a linear SVM to refine and enhance each other. Performing this iteratively produces the desired patch clusters containing similar patches, i.e. patches describing the same visual information. 
\\
\\
Initially, this similarity is unknown and has to be discovered. If some of these patches are known to be similar, these similar patches could be used to train a SVM for classification. Fortunately, a similarity can be produced by clustering the patches using k-means clustering. The problem with the low-level metric used by k-means clustering is that it produces poor results on image patches. To refine the clusters, again, a SVM could be trained to produce a similarity metric. That means, that the clustering depends on the similarity metric of the SVM while the SVM itself depends on the clusters to be trained. To solve this problem the described approach is performed in an iterative way. First, the data is clustered in HOG space and then a SVM is trained. The SVM uses the clusters to produce a metric which then is used to refine the clusters. This is repeated until a convergence criterion is reached, for example that the best clusters identified so far do not change anymore. The classifiers found are ranked according to their purity and discriminativeness. Singh et Al. \cite{Singh2012DiscPat} define purity as the sum of the classifiers detection scores of the top cluster members. Discriminativeness describes the ratio of detections on the positive training data and the union of the positive and negative training data. This ratio should be low but still above 0. Finally, the best classifier ranked like this is built upon and identifies the most discriminative patch that can be found in the original images.

\subsection{Problem of Overfitting}

The drawback of the process described is that the classifier might suffer from overfitting. Overfitting occurs, for example, when the number of features exceeds the number of training data. In this case, the classifier does not predict by generalizing from what he has learned but memorizes input patterns and recalls them. This effect can be measured by comparing the accuracy of the prediction of training data with the one of unknown validation data. In this case the prediction of training samples is far more accurate than the prediction of unknown validation data. To prevent this overfitting the input data of the algorithm can be divided into two datasets initially. One of these two datasets will then act as training data and the other as validation data. In each iteration of the algorithm, the datasets are exchanged for the next iteration to enhance the clusters by new feature patches. This means, the SVM is trained with the training data and the trained classifiers are then used to predict the samples of the validation data. The patches in the validation dataset, that are classified with the best score, will be added to the already obtained clusters to ensure more diversity. This is repeated in the next iteration using the validation data for training and the training data for validation. Overfitting is a particular problem for this project, as on the one hand the features need a certain minimal amount of detail for the patches to be distinguishable. On the other hand it is difficult to obtain enough data to train the SVMs, as human facial expressions are retrieved from images of humans. As humans need to explicitly agree 
with the usage of their personal data and additionally the expressions need to be expressive enough, data sets for this kind of problem are few in amount.
